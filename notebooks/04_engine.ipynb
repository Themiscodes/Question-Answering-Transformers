{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Engine\n",
    "\n",
    "## 04: Question Answering Modules\n",
    "\n",
    "For the modules\n",
    "\n",
    " to download the necessary files and initialise the span entity and relation prediction models, as\n",
    "\n",
    "\n",
    "- I added colour in the messages and progress bars to make the interface a bit more appealing. \n",
    "- The preprocessing of the question is similar to the one I used for the data ingestion, for example removing accents, from Adèle to adele. \n",
    "- For the models the forward method are used to get the logits and then the highest probabilites are the predictions.\n",
    "- For the SPARQL builder and executor modules I used the suggestions provided by the SPARQL documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlease wait...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Knowledge Base: 100%|██████████| 24103/24103 [01:22<00:00, 292.17it/s]\n",
      "Loading Entity & Relation Model: 100%|██████████| 2/2 [00:04<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mEverything is up and running!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ANSI colour codes\n",
    "RED = '\\033[91m'\n",
    "GREEN = '\\033[92m'\n",
    "YELLOW = '\\033[93m'\n",
    "BLUE = '\\033[94m'\n",
    "MAGENTA = '\\033[95m'\n",
    "CYAN = '\\033[96m'\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "# Message to the user\n",
    "print(GREEN + \"Please wait...\" + RESET)\n",
    "\n",
    "# Importing libraries\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import difflib\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "from transformers import BertTokenizer, BertModel, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Initialise variables and GPU\n",
    "device = 'mps' if (torch.backends.mps.is_available()) else 'cuda' if ( torch.cuda.is_available()) else 'cpu'\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n",
    "\n",
    "# Firstly read the dictionary I created\n",
    "df = pd.read_csv(\"dataset/entity_dict.csv\", sep = ',')\n",
    "Entities = df['Entity']\n",
    "Entity_ids = df['Id']\n",
    "\n",
    "# Remove accents from the 'Entity' column\n",
    "df['Entity'] = df['Entity'].apply(lambda x: unidecode(x))\n",
    "\n",
    "# Create a dictionary with entities as keys and entity_ids as values\n",
    "entity_to_id = {entity: entity_id for entity, entity_id in zip(df['Entity'], df['Id'])}\n",
    "entity_list = list(entity_to_id.keys())\n",
    "\n",
    "# This dictionary for similar words for extreme cases\n",
    "entity_docs = {}\n",
    "\n",
    "# Create a dictionary with docs as keys and entity_ids as values\n",
    "for entity_name, id in tqdm(entity_to_id.items(), desc='Building Knowledge Base'):\n",
    "    doc = nlp(entity_name)\n",
    "    if doc.has_vector:\n",
    "        entity_docs[doc] = id\n",
    "\n",
    "# Then read the relations\n",
    "df = pd.read_csv(\"dataset/relation_vocab.csv\")\n",
    "\n",
    "# Create a list with the relation vocabulary\n",
    "relation_vocab = df['Relation'].to_list()\n",
    "\n",
    "del df\n",
    "\n",
    "class BERT_SPAN(torch.nn.Module):\n",
    "    def __init__(self, bert_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "        self.start_head = nn.Sequential(\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.Linear(self.bert.config.hidden_size, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.end_head = nn.Sequential(\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.Linear(self.bert.config.hidden_size, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        start_ent = self.start_head(sequence_output)\n",
    "        end_ent = self.end_head(sequence_output)\n",
    "        \n",
    "        return start_ent * attention_mask, end_ent * attention_mask\n",
    "\n",
    "class BERT_REL(torch.nn.Module):\n",
    "    def __init__(self, bert_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "        self.relation_head = nn.Sequential(\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(self.bert.config.hidden_size, vocab_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        ) \n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)        \n",
    "        return  self.relation_head(outputs[1])\n",
    "\n",
    "def preprocess(question):\n",
    "    question = unidecode(question.replace(\"?\", \"\").replace(\"'s\", \"\"))\n",
    "    ids = []\n",
    "    mask = []\n",
    "    \n",
    "    # Encode the question\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text = question,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    ids.append(encoding['input_ids'])\n",
    "    mask.append(encoding['attention_mask'])\n",
    "\n",
    "    return torch.tensor(ids), torch.tensor(mask)\n",
    "\n",
    "def relation_prediction(model, ids, mask, relation_vocab):\n",
    "        \n",
    "    # Predict \n",
    "    relation_logits = model(input_ids=ids, attention_mask=mask)\n",
    "    _ , prediction = torch.max(relation_logits, dim=1)\n",
    "    \n",
    "    return relation_vocab[prediction]\n",
    "\n",
    "# In case the entity is slightly off by a word or summat\n",
    "def find_closest_match(entity, entity_docs, entity_to_id, entity_list):\n",
    "        \n",
    "    doc = nlp(entity)\n",
    "    if doc.has_vector:\n",
    "\n",
    "        scores = {}\n",
    "        # Find similarity scores between entity and each entity in dictionary\n",
    "        for doc_ent, id in entity_docs.items():\n",
    "            scores[id] = doc.similarity(doc_ent)\n",
    "\n",
    "        best_score = max(scores.values())\n",
    "        return [id for id, score in scores.items() if score == best_score][0].split()[0]\n",
    "    \n",
    "    # Else simply find the best match from the rest\n",
    "    best_match = difflib.get_close_matches(entity, entity_list, n=1, cutoff=0.12)\n",
    "    if best_match != []:\n",
    "        return entity_to_id[best_match[0]][0]\n",
    "    return None\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "def entity_prediction(model, ids, mask, entity_docs, entity_to_id, entity_list):\n",
    "\n",
    "    # Probabilities from the model\n",
    "    start_logits, end_logits = model(input_ids=ids, attention_mask=mask)\n",
    "\n",
    "    # Find the start index with the highest probability\n",
    "    start_pred = torch.argmax(start_logits, dim=1)\n",
    "\n",
    "    # Create a mask with the same shape as the matrix\n",
    "    start_mask = torch.zeros(len(mask[0])).to(device)\n",
    "    start_mask[start_pred.item():] = 1\n",
    "\n",
    "    # Find the end index with the highest probability\n",
    "    masked_end_logits = end_logits * start_mask\n",
    "    end_pred = torch.argmax(masked_end_logits, dim=1)\n",
    "\n",
    "    # Get the entity from the tokens\n",
    "    tokens = ids[0][start_pred.item():end_pred.item()+1]\n",
    "    entity = tokenizer.decode(tokens)\n",
    "\n",
    "    # Initially check if it's in its current form (without accents etc)\n",
    "    if entity in entity_to_id:\n",
    "        return entity_to_id[entity]\n",
    "    else:\n",
    "        entityid = get_entityid(entity)\n",
    "\n",
    "        if entityid != []:\n",
    "            return entityid[0]\n",
    "\n",
    "    # Fallback to the most similar from the dictionary\n",
    "    return find_closest_match(entity, entity_docs, entity_to_id, entity_list)\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "def get_entityid(label):\n",
    "    query = \"\"\"SELECT distinct ?item ?itemLabel ?itemDescription WHERE{  \n",
    "    ?item ?label \"%s\"@en.\n",
    "    ?article schema:about ?item .\n",
    "    ?article schema:inLanguage \"en\" .\n",
    "    ?article schema:isPartOf <https://en.wikipedia.org/>.\t\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }    \n",
    "    }\"\"\" % (label.title())\n",
    "\n",
    "    results = get_results(endpoint_url, query)\n",
    "\n",
    "    entityids = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        entityids.append(result[\"item\"][\"value\"].split(sep='/')[-1])\n",
    "    return entityids\n",
    "\n",
    "def query_builder(entityid, relation):\n",
    "\n",
    "    # Check whether the question relation is inverse\n",
    "    inverse = False\n",
    "    if (relation[0] == 'R'):\n",
    "        relation = relation.replace('R', 'P')\n",
    "        inverse = True\n",
    "\n",
    "    if inverse:\n",
    "        query = \"\"\"\n",
    "        SELECT ?item ?itemLabel \n",
    "        WHERE \n",
    "        {\n",
    "        ?item wdt:%s wd:%s.\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "        }\"\"\" % ( relation, entityid)\n",
    "    else:\n",
    "        query = \"\"\"\n",
    "        SELECT ?item ?itemLabel \n",
    "        WHERE \n",
    "        {\n",
    "        wd:%s wdt:%s ?item.\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "        }\"\"\"  % (entityid, relation)\n",
    "\n",
    "    return query\n",
    "\n",
    "def query_executor(query):\n",
    "    \n",
    "    endpoint = \"https://query.wikidata.org/sparql\"\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    sparql = SPARQLWrapper(endpoint, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    return results[\"results\"][\"bindings\"]\n",
    "\n",
    "\n",
    "def answer(question):\n",
    "    print(CYAN + question + RESET)\n",
    "    ids, mask = preprocess(question)\n",
    "    ids = ids.to(device)\n",
    "    mask = mask.to(device)\n",
    "    relation = relation_prediction(relation_model, ids, mask, relation_vocab)\n",
    "    entity = entity_prediction(entity_model, ids, mask, entity_docs, entity_to_id, entity_list)\n",
    "    if entity == None:\n",
    "        print(MAGENTA + \"Nothing found, I'm sorry...\" + RESET)\n",
    "    else:\n",
    "        query = query_builder(entityid=entity, relation=relation)\n",
    "        results = query_executor(query)\n",
    "        if results== []:\n",
    "            print(MAGENTA + \"Nothing found, I'm sorry...\" + RESET)\n",
    "        for iter, result in enumerate(results):\n",
    "\n",
    "            # To only print top 5 results\n",
    "            if iter == 5:\n",
    "                break\n",
    "            # Print the answer\n",
    "            print(YELLOW + result[\"itemLabel\"][\"value\"] + RESET)\n",
    "\n",
    "models = []\n",
    "for i in tqdm(range(2) , desc='Loading Entity & Relation Model'):\n",
    "\n",
    "    if i ==0:\n",
    "        entity_model = BERT_SPAN(bert_model=BERT_MODEL, vocab_size=len(relation_vocab)).to(device)\n",
    "        entity_model.load_state_dict(torch.load('./best_span_model.pt'))\n",
    "    else:\n",
    "        relation_model = BERT_REL(bert_model=BERT_MODEL, vocab_size=len(relation_vocab)).to(device)\n",
    "        relation_model.load_state_dict(torch.load('./best_relation_model.pt'))\n",
    "\n",
    "print(GREEN + \"Everything is up and running!\" + RESET)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "When asking questions from the train or validation set the model performed brilliantly. From the test set it struggled in only a few questions, but overall it had no problem like for example in the question below from the test set. This hasn't been used for training so it should give an accurate indication that the model is working correctly on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mwhat position does jose francisco torres play\u001b[0m\n",
      "\u001b[93mmidfielder\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"what position does jose francisco torres play\"\n",
    "answer(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is predicting accurately the entity and relation, and the answer is correct. Then below I try with a question that isn't in the test set, by modifying the above question's context to see if it can identify that the relation is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mwhich national team does francisco torres play for?\u001b[0m\n",
      "\u001b[93mUnited States of America\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"which national team does francisco torres play for?\"\n",
    "answer(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation model correctly identifies that it's a different relation and answers the question correctly. The interesting thing is that even by altering the name and removing the first name (ie Jose) of the footballer the model is able to predict really well and identify the correct entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mWhat does vettel do?\u001b[0m\n",
      "\u001b[93mracing automobile driver\u001b[0m\n",
      "\u001b[93mFormula One driver\u001b[0m\n",
      "\u001b[93mmotorsports competitor\u001b[0m\n",
      "\u001b[93minternational forum participant\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"What does vettel do?\"\n",
    "answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mWhere is Max Verstappen from?\u001b[0m\n",
      "\u001b[93mBelgium\u001b[0m\n",
      "\u001b[93mKingdom of the Netherlands\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"Where is max verstappen from?\"\n",
    "answer(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two examples above the entities Vettel or Verstappen aren't present in the train set and subsequently in the entity dictionary, but the model is still able to predict the correct span entity and provide the right answer to the question. This is achieved by locating the label and using it to identify the relevant entity id with a query. \n",
    "\n",
    "This makes the model able to answer questions on entities that weren't used for training. Below you can experiment by entering a question and running the cell to get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\n",
    "answer(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "639c2a9cefc4711935c69ef9458e0ee4184e18203870c7a492801c98b906b529"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
