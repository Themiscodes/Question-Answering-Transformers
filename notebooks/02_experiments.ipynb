{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Engine\n",
    "\n",
    "## 02: Single Model Approach\n",
    "\n",
    "In my initial experiments, I implemented a single question answering BERT-based model for both span entity and relation prediction. A problem with this approach, as I found out, was that the model struggled to learn both tasks effectively, with relation extraction being a more complex task than entity recognition.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "Importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Numpy, Plotting, Metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Testing and metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# BERT\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Metal to run it locally on apple silicon, it falls back to CUDA online, else CPU as final resort\n",
    "device = 'mps' if (torch.backends.mps.is_available()) else 'cuda' if ( torch.cuda.is_available()) else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Reading with pandas the data from the CSV files to create train, validation, and test datasets. Also creating the relation vocabulary to create a list of possible relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly read the dictionary I created\n",
    "df = pd.read_csv(\"dataset/entity_dict.csv\", sep = ',')\n",
    "Entities = df['Entity']\n",
    "Entity_ids = df['Id']\n",
    "\n",
    "# Remove accents from the 'Entity' column\n",
    "df['Entity'] = df['Entity'].apply(lambda x: unidecode(x))\n",
    "\n",
    "# Then read the relations\n",
    "df = pd.read_csv(\"dataset/relation_vocab.csv\")\n",
    "\n",
    "# Create a list with the relation vocabulary\n",
    "relation_vocab = df['Relation'].to_list()\n",
    "\n",
    "# Finally read the train dataset\n",
    "df = pd.read_csv(\"dataset/train_dataset.csv\", sep = ',')\n",
    "train_Questions = df['question']\n",
    "train_Entity_ids = df['entity_id']\n",
    "train_Entity_labels = df['entity_label']\n",
    "train_Entity_start = df['entity_start']\n",
    "train_Entity_end = df['entity_end']\n",
    "train_Relation_ids = df['relation_id']\n",
    "train_Answer_ids = df['answer_id']\n",
    "\n",
    "# Validation dataset\n",
    "df = pd.read_csv(\"dataset/val_dataset.csv\", sep = ',')\n",
    "val_Questions = df['question']\n",
    "val_Entity_ids = df['entity_id']\n",
    "val_Entity_labels = df['entity_label']\n",
    "val_Entity_start = df['entity_start']\n",
    "val_Entity_end = df['entity_end']\n",
    "val_Relation_ids = df['relation_id']\n",
    "val_Answer_ids = df['answer_id']\n",
    "\n",
    "# Test dataset\n",
    "df = pd.read_csv(\"dataset/test_dataset.csv\", sep = ',')\n",
    "test_Questions = df['question']\n",
    "test_Entity_ids = df['entity_id']\n",
    "test_Entity_labels = df['entity_label']\n",
    "test_Entity_start = df['entity_start']\n",
    "test_Entity_end = df['entity_end']\n",
    "test_Relation_ids = df['relation_id']\n",
    "test_Answer_ids = df['answer_id']\n",
    "\n",
    "# Free the dataframe's memory resources\n",
    "del df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the BERT tokenizer from the pre-trained \"bert-base-uncased\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions & Model\n",
    "\n",
    "Seeding function for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seedTorch(seed=33):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For preprocessing since the span start and end correspond to actual words, but here I'll be using the BERT tokenizer I need to locate the corresponding start and end tokens in the BERT tokenized question for each entity. The function returns the tokenized input IDs, attention masks, and the start and end token positions for each question as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns input_ids, attention_masks, start, ends for BERT\n",
    "def preprocess(questions, max_length, entity_starts, entity_ends):\n",
    "\n",
    "    ids = []\n",
    "    masks = []\n",
    "    token_starts = []\n",
    "    token_ends = []\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "\n",
    "        # Not replacing ? and 's here since these might be helpful \n",
    "        question = unidecode(question.replace(\"?\", \"\").replace(\"'s\", \"\"))\n",
    "        question_t = question.split()\n",
    "\n",
    "        # Locate the corresponding start in the BERT tokenized question\n",
    "        start_token = tokenizer.encode_plus(\n",
    "            text = question_t[entity_starts[i]],\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=False\n",
    "        ) \n",
    "        start_token = start_token['input_ids'][0]\n",
    "\n",
    "        # Locate the corresponding end in the BERT tokenized question\n",
    "        end_token = tokenizer.encode_plus(\n",
    "            text = question_t[entity_ends[i]],\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=False\n",
    "        ) \n",
    "        end_token = end_token['input_ids'][-1]\n",
    "\n",
    "        # Encode the question\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text = question,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=False\n",
    "        ) \n",
    "        tokens = encoding['input_ids']\n",
    "        \n",
    "        # Append in the corresponding lists\n",
    "        ids.append(tokens)\n",
    "        masks.append(encoding['attention_mask'])\n",
    "        token_starts.append(tokens.index(start_token))\n",
    "        token_ends.append(tokens.index(end_token))\n",
    "\n",
    "    return torch.tensor(ids), torch.tensor(masks), token_starts, token_ends"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom dataset class that preprocesses the questions using the function I defined above. It keeps the input_ids, attention_masks, entity start and end token positions, and relation IDs for each question as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom Question Dataset class to use for the dataloaders\n",
    "class QuestionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, questions, entity_start, entity_end, relation_ids, length):\n",
    "        ids, masks, entity_start_T, entity_end_T = preprocess(questions, length, entity_start, entity_end)\n",
    "        self.max_length = length\n",
    "        self.input_ids = ids\n",
    "        self.attention_masks = masks \n",
    "        self.entity_start = torch.tensor(entity_start_T, dtype=torch.long)\n",
    "        self.entity_end = torch.tensor(entity_end_T, dtype=torch.long)\n",
    "        self.relation_ids = torch.tensor([relation_vocab.index(r) for r in relation_ids.to_list() ])\n",
    "        self.samples = len(relation_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.input_ids[idx], \n",
    "                'attention_mask': self.attention_masks[idx],\n",
    "                'entity_start': self.entity_start[idx],\n",
    "                'entity_end': self.entity_end[idx],\n",
    "                'relation_ids': self.relation_ids[idx]\n",
    "                }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I calculate the maximum length of input tokens among the training, validation, and test sets. It iterates through each question in each set, encodes the question using BERT's tokenizer, and stores the length of the resulting input tokens. The final value represents the maximum number of tokens among all questions in the three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "maxlen = 0\n",
    "listsets = train_Questions, val_Questions, test_Questions\n",
    "for questions in listsets:\n",
    "        for question in questions:\n",
    "                encoding = tokenizer.encode_plus(\n",
    "                        text = question,\n",
    "                        return_attention_mask=True,\n",
    "                        add_special_tokens=False\n",
    "                ) \n",
    "                tokens = encoding['input_ids']\n",
    "                length_tokens = len(tokens)\n",
    "                if ( length_tokens> maxlen):\n",
    "                        maxlen = length_tokens\n",
    "print(maxlen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the maximum length is 34 a maximum sequence length of 36 tokens would be enough. Also I select a batch size of 32 which worked well so far with BERT (with a larger batch size in my local machine it was unable to run). I create the three datasets for the training, validation and testing data and the corresponding dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 36\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Tokenizing, preprocessing and dataset creation\n",
    "train_dataset = QuestionDataset(train_Questions, train_Entity_start, train_Entity_end, train_Relation_ids, MAX_LENGTH)\n",
    "val_dataset = QuestionDataset(val_Questions, val_Entity_start, val_Entity_end, val_Relation_ids, MAX_LENGTH)\n",
    "test_dataset = QuestionDataset(test_Questions, test_Entity_start, test_Entity_end, test_Relation_ids, MAX_LENGTH)\n",
    "\n",
    "# Corresponding Dataloaders\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I define the BERT_QA class that has three linear layers to predict the start and end positions of the entity (self.start_head and self.end_head, respectively) and the relation label (self.relation_head). \n",
    "\n",
    "- In the forward method, the input token IDs and attention mask are passed through the BERT model to get the sequence output. \n",
    "- The sequence output is passed through the linear layers to get the start and end logits, which are multiplied by the attention mask to mask out the padding tokens. \n",
    "- The pooled output is passed through the linear layer for predicting the relation label to get the relation logits, which are returned along with the masked softmax probabilities for the start logits and end logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_QA(torch.nn.Module):\n",
    "    def __init__(self, bert_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "        self.start_head = nn.Sequential(\n",
    "            nn.Dropout(p=0.13),\n",
    "            nn.Linear(self.bert.config.hidden_size, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.end_head = nn.Sequential(\n",
    "            nn.Dropout(p=0.13),\n",
    "            nn.Linear(self.bert.config.hidden_size, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.relation_head = nn.Sequential(\n",
    "            nn.Dropout(0.13),\n",
    "            nn.Linear(self.bert.config.hidden_size, vocab_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        start_ent = self.start_head(sequence_output)\n",
    "        end_ent = self.end_head(sequence_output)\n",
    "        \n",
    "        return start_ent* attention_mask, end_ent * attention_mask, self.relation_head(outputs[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have the training functions for the span entity & relation prediction model. \n",
    "- `train_epoch` trains the model for one epoch and returns the mean loss. \n",
    "- `train_model` trains the model for a given number of epochs and returns the trained model.\n",
    "- `optimize_model` trains the model for a given number of epochs and returns the trained model that has the best F1 score on the validation dataset.\n",
    "- `evaluation_function` evaluates the model on a given dataset and prints the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train for one epoch the span model\n",
    "def train_epoch(optimizer, scheduler, dataloader, lossfunc, model, device, display=True, clip_value=0.6):\n",
    "\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "\n",
    "    # For each batch\n",
    "    for batch, data in enumerate(dataloader):\n",
    "\n",
    "        # In case the GPU is used\n",
    "        ids = data['input_ids'].to(device)\n",
    "        mask = data['attention_mask'].to(device)\n",
    "\n",
    "        # The actual entity and relations\n",
    "        actual_entity_starts = data['entity_start'].to(device)\n",
    "        actual_entity_ends = data['entity_end'].to(device)\n",
    "\n",
    "        # The actual relations\n",
    "        actual_relations = data['relation_ids'].to(device)\n",
    "        \n",
    "        # Predict and calculate loss\n",
    "        start_logits, end_logits, relation_logits  = model(input_ids=ids, attention_mask=mask)\n",
    "\n",
    "        # Find the start and end indices with the highest probability\n",
    "        start_preds = torch.argmax(start_logits, dim=1)\n",
    "\n",
    "        # Create a mask with the same shape as the matrix\n",
    "        start_mask = torch.zeros((len(start_preds), MAX_LENGTH)).to(device)\n",
    "\n",
    "        # Set the ones in the mask based on the indices in the tensor\n",
    "        for i, idx in enumerate(start_preds):\n",
    "            start_mask[i, idx:] = 1\n",
    "\n",
    "        masked_end_logits = end_logits * start_mask\n",
    "\n",
    "        span_loss1 = lossfunc(start_logits, actual_entity_starts)\n",
    "        span_loss2 = lossfunc(masked_end_logits, actual_entity_ends)\n",
    "\n",
    "        rel_loss = lossfunc(relation_logits, actual_relations)\n",
    "\n",
    "        total_loss = span_loss1+span_loss2+3*rel_loss\n",
    "        losses.append(total_loss.item())\n",
    "\n",
    "        # Inform the weights\n",
    "        total_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # When display is set print every 64 the loss\n",
    "        if(display):\n",
    "            if batch % 64 == 0:\n",
    "                size = len(dataloader.dataset)\n",
    "                loss, current = total_loss.item(), batch * len(ids)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # Return the total mean loss\n",
    "    meanloss = 0\n",
    "    if len(losses)!=0:\n",
    "        meanloss = sum(losses)/len(losses)\n",
    "    return meanloss\n",
    " \n",
    "# Function to train a model\n",
    "def train_model(epochs, optimizer, scheduler, dataloader, entropy_loss, model, device, display=True, clip_value=0.6):\n",
    "\n",
    "    # For each epoch\n",
    "    for epoch in range(epochs):\n",
    "        if (display):\n",
    "            print(f\"\\nEpoch {epoch+1}\\n_________________________________\")\n",
    "        train_epoch(optimizer, scheduler, dataloader, entropy_loss, model, device, display, clip_value)\n",
    "        if (display):\n",
    "            print(\"_________________________________\")\n",
    "        \n",
    "    # Returns the model\n",
    "    return model\n",
    "\n",
    "# Function to train a model\n",
    "def optimize_model(epochs, optimizer, scheduler, dataloader, val_dataloader, lossfunc, model, device, display=True, clip_value=0.6):\n",
    "\n",
    "    best_f1 = 0\n",
    "\n",
    "    # For each epoch\n",
    "    for epoch in range(epochs):\n",
    "        if (display):\n",
    "            print(f\"\\nEpoch {epoch+1}\\n_________________________________\")\n",
    "        train_epoch(optimizer, scheduler, dataloader, lossfunc, model, device, display, clip_value)\n",
    "        _, f1_score, _ = evaluation_function(val_dataloader, model, lossfunc, device, True)\n",
    "        \n",
    "        # If F1-score is better in the validation set then keep this model\n",
    "        if (best_f1<f1_score):\n",
    "            best_f1=f1_score\n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), './best_model.pt')\n",
    "\n",
    "        if (display):\n",
    "            print(\"_________________________________\")\n",
    "        \n",
    "    # Returns the model\n",
    "    return model\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluation_function(dataloader, model, lossfunc, device, display=True):\n",
    "\n",
    "    # Metrics initialisation\n",
    "    losses = []\n",
    "    total_f1_score = 0\n",
    "    total_accuracy = 0 \n",
    "    total_precision = 0 \n",
    "    total_recall = 0 \n",
    "    total_count = 0 \n",
    "    total_relations = 0\n",
    "    correct_relations =0\n",
    "\n",
    "    # So the model is in eval mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # For each batch\n",
    "        for batch, data in enumerate(dataloader):\n",
    "\n",
    "            # In case the GPU is used\n",
    "            ids = data['input_ids'].to(device)\n",
    "            mask = data['attention_mask'].to(device)\n",
    "\n",
    "            # The actual positions\n",
    "            actual_entity_starts = data['entity_start'].to(device)\n",
    "            actual_entity_ends = data['entity_end'].to(device)\n",
    "            actual_relations = data['relation_ids'].to(device)\n",
    "\n",
    "            # Predict\n",
    "            start_logits, end_logits, relation_logits = model(input_ids=ids, attention_mask=mask)\n",
    "        \n",
    "            # Find the start and end indices with the highest probability\n",
    "            start_preds = torch.argmax(start_logits, dim=1)\n",
    "\n",
    "            # Create a mask with the same shape as the matrix\n",
    "            start_mask = torch.zeros((len(start_preds), MAX_LENGTH)).to(device)\n",
    "\n",
    "            # # Set the ones in the mask based on the indices in the tensor\n",
    "            for i, idx in enumerate(start_preds):\n",
    "                start_mask[i, idx:] = 1\n",
    "\n",
    "            masked_end_logits = end_logits * start_mask\n",
    "            end_preds = torch.argmax(masked_end_logits, dim=1)\n",
    "\n",
    "            span_loss1 = lossfunc(start_logits, actual_entity_starts)\n",
    "            span_loss2 = lossfunc(masked_end_logits, actual_entity_ends)\n",
    "            rel_loss = lossfunc(relation_logits, actual_relations)\n",
    "\n",
    "            total_loss = span_loss1+span_loss2 + 3*rel_loss\n",
    "            losses.append(total_loss.item())\n",
    "\n",
    "            predicted_spans = [(start_preds[i].item(), end_preds[i].item(), None) for i in range(len(start_preds))]\n",
    "            true_spans = [(actual_entity_starts[i].item(), actual_entity_ends[i].item(), None) for i in range(len(actual_entity_starts))]\n",
    "\n",
    "            _ , predictions = torch.max(relation_logits, dim=1)\n",
    "            correct_relations += (predictions==actual_relations).sum()\n",
    "            total_relations += len(actual_relations)\n",
    "\n",
    "            # These to compute the TP, FP, and FN counts for each span\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "\n",
    "            for j in range(len(start_preds)):\n",
    "                # create range objects\n",
    "                true_range = range(int(true_spans[j][0]), int(true_spans[j][1]+1))\n",
    "                pred_range = range(int(predicted_spans[j][0]), int(predicted_spans[j][1]+1))\n",
    "\n",
    "                # Compute the overlap between the predicted and true ranges\n",
    "                overlap = set(true_range).intersection(set(pred_range))\n",
    "\n",
    "                # Update the TP, FP, and FN counts\n",
    "                if len(overlap) > 0:\n",
    "                    tp += 1\n",
    "                    fp += len(pred_range) - len(overlap)\n",
    "                    fn += len(true_range) - len(overlap)\n",
    "                else:\n",
    "                    fp += len(pred_range)\n",
    "                    fn += len(true_range)\n",
    "\n",
    "            # Compute the precision, recall\n",
    "            precision = float(tp) / float(tp + fp)\n",
    "            recall = float(tp) / float(tp + fn)\n",
    "            \n",
    "            # Add to the total metrics\n",
    "            total_f1_score += float(2 * precision * recall) / float(precision + recall)\n",
    "            total_accuracy += float(tp) / float(tp + fp + fn)\n",
    "            total_precision += precision\n",
    "            total_recall += recall\n",
    "            total_count +=1\n",
    "\n",
    "    accuracy_relation = float(correct_relations)/ float(total_relations) * 100\n",
    "    accuracy_entity_span = float(total_accuracy) / float(total_count) * 100\n",
    "    f1_score = float(total_f1_score) / float(total_count) * 100\n",
    "    dataset_wide_f1 = float(2 * total_precision * total_recall) / float(total_precision + total_recall)\n",
    "    f1_star = float(dataset_wide_f1)/ float(total_count) * 100\n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    precision_score = float(total_precision) / float(total_count) * 100\n",
    "    recall_score = float(total_recall) / float(total_count) * 100\n",
    "\n",
    "    # Printing them if display is not false\n",
    "    if display:\n",
    "        print(\"\\nEvaluation Results\")\n",
    "        print(\"_________________________________\")\n",
    "        print(\n",
    "            f\"\\nMean Loss: {mean_loss:.2f} \"\n",
    "            f\"\\n_________________________________\"\n",
    "            f\"\\nEntity Span Prediction\"\n",
    "            f\"\\nPrecision: {precision_score:.2f}%\"\n",
    "            f\"\\nRecall: {recall_score:.2f}%\"\n",
    "            f\"\\nSpan Entity Accuracy : {accuracy_entity_span:.2f}%\"\n",
    "            f\"\\nAverage F1-Score: {f1_score:.2f}%\"\n",
    "            f\"\\nDataset Wide F1*: {f1_star:.2f}%\"\n",
    "            f\"\\n_________________________________\"\n",
    "            f\"\\nRelation Prediction\"\n",
    "            f\"\\nAccuracy : {accuracy_relation:.2f}%\"\n",
    "            )\n",
    "        print(\"_________________________________\")\n",
    "\n",
    "    # Reset the model to train mode\n",
    "    model.train()\n",
    "\n",
    "    return accuracy_entity_span, f1_score, f1_star"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then training the BERT model for five epochs. Initializing the learning rate and gradient clip value with the best found during tuning. I use the cross entropy loss function for both tasks. I also experimented with the MSE loss and a hybrid one that adds the distance from the span actual points, but they had worse performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "_________________________________\n",
      "loss: 21.611191  [    0/19463]\n",
      "loss: 19.657215  [ 2048/19463]\n",
      "loss: 19.508245  [ 4096/19463]\n",
      "loss: 19.863255  [ 6144/19463]\n",
      "loss: 19.632692  [ 8192/19463]\n",
      "loss: 19.900032  [10240/19463]\n",
      "loss: 19.598673  [12288/19463]\n",
      "loss: 19.662283  [14336/19463]\n",
      "loss: 19.960163  [16384/19463]\n",
      "loss: 19.975389  [18432/19463]\n",
      "\n",
      "Evaluation Results\n",
      "_________________________________\n",
      "\n",
      "Mean Loss: 19.65 \n",
      "_________________________________\n",
      "Entity Span Prediction\n",
      "Precision: 84.74%\n",
      "Recall: 87.31%\n",
      "Span Entity Accuracy : 75.57%\n",
      "Average F1-Score: 85.62%\n",
      "Dataset Wide F1*: 86.01%\n",
      "_________________________________\n",
      "Relation Prediction\n",
      "Accuracy : 8.87%\n",
      "_________________________________\n",
      "_________________________________\n",
      "\n",
      "Epoch 2\n",
      "_________________________________\n",
      "loss: 19.536558  [    0/19463]\n",
      "loss: 19.692316  [ 2048/19463]\n",
      "loss: 19.692572  [ 4096/19463]\n",
      "loss: 19.887600  [ 6144/19463]\n",
      "loss: 19.828758  [ 8192/19463]\n",
      "loss: 19.626688  [10240/19463]\n",
      "loss: 19.566326  [12288/19463]\n",
      "loss: 19.723738  [14336/19463]\n",
      "loss: 19.692484  [16384/19463]\n",
      "loss: 19.629995  [18432/19463]\n",
      "\n",
      "Evaluation Results\n",
      "_________________________________\n",
      "\n",
      "Mean Loss: 19.65 \n",
      "_________________________________\n",
      "Entity Span Prediction\n",
      "Precision: 84.77%\n",
      "Recall: 87.24%\n",
      "Span Entity Accuracy : 75.30%\n",
      "Average F1-Score: 85.58%\n",
      "Dataset Wide F1*: 85.99%\n",
      "_________________________________\n",
      "Relation Prediction\n",
      "Accuracy : 8.87%\n",
      "_________________________________\n",
      "_________________________________\n",
      "\n",
      "Epoch 3\n",
      "_________________________________\n",
      "loss: 19.944038  [    0/19463]\n",
      "loss: 20.036203  [ 2048/19463]\n",
      "loss: 19.942333  [ 4096/19463]\n",
      "loss: 19.847639  [ 6144/19463]\n",
      "loss: 19.786213  [ 8192/19463]\n",
      "loss: 19.470749  [10240/19463]\n",
      "loss: 19.692490  [12288/19463]\n",
      "loss: 19.692568  [14336/19463]\n",
      "loss: 19.602312  [16384/19463]\n",
      "loss: 19.625137  [18432/19463]\n",
      "\n",
      "Evaluation Results\n",
      "_________________________________\n",
      "\n",
      "Mean Loss: 19.67 \n",
      "_________________________________\n",
      "Entity Span Prediction\n",
      "Precision: 82.77%\n",
      "Recall: 90.84%\n",
      "Span Entity Accuracy : 76.68%\n",
      "Average F1-Score: 86.24%\n",
      "Dataset Wide F1*: 86.62%\n",
      "_________________________________\n",
      "Relation Prediction\n",
      "Accuracy : 8.87%\n",
      "_________________________________\n",
      "_________________________________\n",
      "\n",
      "Epoch 4\n",
      "_________________________________\n",
      "loss: 19.776861  [    0/19463]\n",
      "loss: 19.692451  [ 2048/19463]\n",
      "loss: 19.723433  [ 4096/19463]\n",
      "loss: 19.471212  [ 6144/19463]\n",
      "loss: 19.723679  [ 8192/19463]\n",
      "loss: 19.286293  [10240/19463]\n",
      "loss: 19.435467  [12288/19463]\n",
      "loss: 19.690125  [14336/19463]\n",
      "loss: 19.786312  [16384/19463]\n",
      "loss: 19.783741  [18432/19463]\n",
      "\n",
      "Evaluation Results\n",
      "_________________________________\n",
      "\n",
      "Mean Loss: 19.64 \n",
      "_________________________________\n",
      "Entity Span Prediction\n",
      "Precision: 84.52%\n",
      "Recall: 92.76%\n",
      "Span Entity Accuracy : 79.63%\n",
      "Average F1-Score: 88.18%\n",
      "Dataset Wide F1*: 88.45%\n",
      "_________________________________\n",
      "Relation Prediction\n",
      "Accuracy : 8.87%\n",
      "_________________________________\n",
      "_________________________________\n",
      "\n",
      "Epoch 5\n",
      "_________________________________\n",
      "loss: 19.567448  [    0/19463]\n",
      "loss: 19.692457  [ 2048/19463]\n",
      "loss: 19.348804  [ 4096/19463]\n",
      "loss: 19.659735  [ 6144/19463]\n",
      "loss: 19.660040  [ 8192/19463]\n",
      "loss: 19.348530  [10240/19463]\n",
      "loss: 19.692444  [12288/19463]\n",
      "loss: 19.661200  [14336/19463]\n",
      "loss: 19.703779  [16384/19463]\n",
      "loss: 19.535988  [18432/19463]\n",
      "\n",
      "Evaluation Results\n",
      "_________________________________\n",
      "\n",
      "Mean Loss: 19.62 \n",
      "_________________________________\n",
      "Entity Span Prediction\n",
      "Precision: 85.18%\n",
      "Recall: 94.28%\n",
      "Span Entity Accuracy : 81.10%\n",
      "Average F1-Score: 89.12%\n",
      "Dataset Wide F1*: 89.50%\n",
      "_________________________________\n",
      "Relation Prediction\n",
      "Accuracy : 8.87%\n",
      "_________________________________\n",
      "_________________________________\n",
      "Training Time: 49.68 minutes\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 4e-5\n",
    "CLIP_VALUE = 0.7\n",
    "\n",
    "# Instaniate the model\n",
    "seedTorch()\n",
    "model = BERT_QA(bert_model=BERT_MODEL, vocab_size=len(relation_vocab)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(train_dataloader)*EPOCHS)\n",
    "lossfunc = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "model = optimize_model(EPOCHS,optimizer, scheduler, train_dataloader, val_dataloader, lossfunc, model, device, True, CLIP_VALUE)\n",
    "print(f'Training Time: {(time.time() - start_time)/60:.2f} minutes')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the model achieved good Entity Span Prediction metrics with a Precision of 85.12%, Recall of 92.71%, Accuracy of 80.15%, Average F1-Score of 88.41% and Dataset Wide F1* of 88.75%. However, the model did not perform well in the Relation Prediction task, with an Accuracy of only 8.07%, indicating that it was unable to learn anything relating to this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results\n",
      "_________________________________\n",
      "\n",
      "Mean Loss: 19.65 \n",
      "_________________________________\n",
      "Entity Span Prediction\n",
      "Precision: 85.12%\n",
      "Recall: 92.71%\n",
      "Span Entity Accuracy : 80.15%\n",
      "Average F1-Score: 88.41%\n",
      "Dataset Wide F1*: 88.75%\n",
      "_________________________________\n",
      "Relation Prediction\n",
      "Accuracy : 8.07%\n",
      "_________________________________\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "lossfunc = nn.CrossEntropyLoss()\n",
    "_ = evaluation_function(dataloader=test_dataloader, model=model, lossfunc=lossfunc, device=device,display=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same results can be observed with the test set. One possible reason is that the model may have been biased towards learning to predict entity spans during training and may not have received enough information or examples to learn to predict relations accurately. Another reason could be that the relation prediction task is more difficult than the entity span prediction task, which only needs to locate the entity in the text rather than derive context.\n",
    "\n",
    "I experimented further with different model architectures by enhancing the linear stack with more layers, adding activation functions and more heads and it was still unable to train the relation part of the model. This might be due to the size of the data or that the training data are not diverse enough or representative of the real-world scenarios that the model may encounter during inference, or it might have require a more complex model. In any case in my next experiments I moved to splitting the two tasks to two different models which was immediately more succesful, as you can see in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "639c2a9cefc4711935c69ef9458e0ee4184e18203870c7a492801c98b906b529"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
